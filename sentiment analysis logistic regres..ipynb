{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the classes :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the cleaning/preprocessing method :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    tweet1 = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet) #remove hyperlinks present\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True) #initiate tokenize to convert the string into list of words.\n",
    "    tokens = tokenizer.tokenize(tweet1)\n",
    "    stopwords_english = stopwords.words('english') #stopwords like the, is, etc that are irrelevant\n",
    "    clean_tweets = []\n",
    "    for word in tokens:\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            clean_tweets.append(word)\n",
    "        \n",
    "    stemmer = PorterStemmer() #stemmer for stemming words\n",
    "    stemmed_tweets = []\n",
    "    for word in clean_tweets:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        stemmed_tweets.append(stem_word)\n",
    "\n",
    "    return stemmed_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a word frequency dictionary based on the classification :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dict(tweets,ys):\n",
    "    yslist = np.squeeze(ys).tolist() #to prevent the list from ending up with one element\n",
    "    freq = {}\n",
    "    for y,tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word,y)\n",
    "            freq[pair] = freq.get(pair,0)+1\n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function and Gradient Descent :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''x is matrix of features                                 (m,n+1)\n",
    "   y is corresponding labels (positive and negative)       (m,1)\n",
    "   theta is weight vector                                  (n+1,1)\n",
    "   alp is learning rate            \n",
    "   iter is number of interations'''\n",
    "\n",
    "def gradient(x,y,theta,alp,iter):\n",
    "    m = len(x)\n",
    "    for i in range(0,iter):\n",
    "        z = np.dot(x,theta)\n",
    "        h = 1/(1+np.exp(-z))   #sigmoid function\n",
    "        J = (-1/m)*(np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))\n",
    "        theta = theta - (alp/m)*np.dot(x.T,h-y)\n",
    "    plt.plot(z,h)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from the tweets using freq dictionary :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Output as  x: a feature vector of dimension (1,3)\n",
    "\n",
    "def extract_features(tweet,freq):\n",
    "    words = process_tweet(tweet)\n",
    "    x = np.zeros((1,3))   #3 elements in for of a 1X3 vector\n",
    "    x[0,0] = 1            #bias term set to 1\n",
    "    for word in words:\n",
    "        x[0,1] += freq.get((word,1),0)    #increment count for positive label 1\n",
    "        x[0,2] += freq.get((word,0),0)    #increment count for negative label 0\n",
    "    assert(x.shape==(1,3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_new.csv\")\n",
    "train_x = df[\"selected_text\"].values\n",
    "train_y = df[\"sentiment\"].values\n",
    "freqs = freq_dict(train_x,train_y)\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):       #creating a feature vector x for each tweet``\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "# training labels corresponding to X\n",
    "Y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Gradient descent :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply gradient descent\n",
    "J, theta = gradient(X, Y, np.zeros((3, 1)), 1e-7, 1500)\n",
    "print(f\"The cost after training is {J}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction method and testing the model :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    z = np.dot(x,theta)\n",
    "    y_pred = 1/(1+np.exp(-z))\n",
    "    return y_pred\n",
    "    \n",
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "   \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "\n",
    "        if y_pred.any > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "# With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    y_hat = np.array(y_hat)\n",
    "    test_y = test_y.reshape(-1)\n",
    "    accuracy = np.sum((test_y == y_hat).astype(int))/len(test_x)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'builtin_function_or_method' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10568/1668188853.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfreq1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfreq_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy of the model is \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10568/2415543889.py\u001b[0m in \u001b[0;36mtest_logistic_regression\u001b[1;34m(test_x, test_y, freqs, theta)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[1;31m# append 1.0 to the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'builtin_function_or_method' and 'float'"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"test_new.csv\")\n",
    "test_x = test[\"selected_text\"].values\n",
    "test_y = test[\"sentiment\"].values\n",
    "freq1 = freq_dict(test_x,test_y)\n",
    "accuracy = test_logistic_regression(test_x, test_y, freq1, theta)\n",
    "print(\"Accuracy of the model is \" + str(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c3c51bfa33419332fba3e11dcc1a93eea7e3cf5630fb139b8be95bdd6dd3ada"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
